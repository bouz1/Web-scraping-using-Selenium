{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c40bb2f-9ad7-4453-97ee-1463e9a3f70f",
   "metadata": {},
   "source": [
    "<h1 style='text-align: center;'> Web scraping using Selenium </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59385ad6-8d55-444a-a978-38b42c4c5630",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b8c96-04af-4520-86da-4281ae1296b8",
   "metadata": {},
   "source": [
    "This Python project is a web scraping script that extracts job offers from the Indeed website. It uses the **Selenium** and BeautifulSoup libraries to automate the process of entering search queries and extracting data from the website. The script is designed to search for job offers related to a topic and a location. \n",
    "<br><br>\n",
    "**Introduction of ***Selenium*** library** <br> The selenium library is a Python package that provides a simple way to automate web browsers. It is used to interact with web pages and perform tasks such as filling out forms, clicking buttons, and navigating between pages. The library is built on top of the WebDriver API, which is a cross-platform API for controlling web browsers. The selenium library supports several popular web browsers, including Firefox, Chrome, and Internet Explorer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c73397-0903-4aa7-9ddc-7cc325a07273",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd25db12-29b3-4efb-8d3e-f63ce5129b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium and web scraping libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Usual libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import ipywidgets as widgets \n",
    "import numpy as np\n",
    "import warnings\n",
    "import glob \n",
    "\n",
    "#If the chrome didn't open, please see the below explanation\n",
    "#https://stackoverflow.com/questions/29858752/error-message-chromedriver-executable-needs-to-be-available-in-the-path\n",
    "#!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e0529f7a-bf4b-4545-a332-bb6887b7ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warning \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9471c-d12d-4a06-bbd6-181d4b27ccee",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "666ae933-8a03-4b1c-997f-f37932430935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_topic(job_topic_str):\n",
    "    \"\"\"\n",
    "    This function sets the search topic for job offers.\n",
    "\n",
    "    Args:\n",
    "        job_topic_str: A string representing the search topic.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select the input box \"What\"\n",
    "    what=driver.find_element(By.XPATH,\"//input[@id='text-input-what']\")\n",
    "\n",
    "    # Select all the text in the input box\n",
    "    what.send_keys(Keys.CONTROL, 'a')\n",
    "\n",
    "    # Delete the previous writing if exists\n",
    "    what.send_keys(Keys.DELETE)\n",
    "\n",
    "    # Enter the search topic\n",
    "    what.send_keys(job_topic_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d0969526-014f-45b6-bdd9-cede8a15c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_localisation(localisation):\n",
    "    \"\"\"\n",
    "    This function enters the location of the search into the input box \"Where\" on a webpage.\n",
    "\n",
    "    Args:\n",
    "        localisation: A string representing the location of the search.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Select the input box \"Where\"\n",
    "    where=driver.find_element(By.XPATH,\"//input[@id='text-input-where']\")\n",
    "\n",
    "    # Select all the text in the input box\n",
    "    where.send_keys(Keys.CONTROL, 'a')\n",
    "\n",
    "    # Delete the previous writing if exists\n",
    "    where.send_keys(Keys.DELETE)\n",
    "\n",
    "    # Enter the location of the search\n",
    "    where.send_keys(\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "45ed1bb9-7468-425b-827a-9d74f2014d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the job divs, in average each page has 15 job offres \n",
    "def find_all_job_divs():\n",
    "    # Git the page source and convet it to BeautifulSoup\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    # Find the job divs recognised with the class name = \"job_seen_beacon\"\n",
    "    divs_mosaic=soup.find_all('div',class_=\"job_seen_beacon\")\n",
    "    #return the list of the job divs and the soup object\n",
    "    return  divs_mosaic, soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65449880-a1f4-429d-9ac2-374d869fdbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_title(div):\n",
    "    \"\"\"\n",
    "    This function reads the job title from a job div.\n",
    "\n",
    "    Args:\n",
    "        div: A BeautifulSoup object representing a job div.\n",
    "\n",
    "    Returns:\n",
    "        The job title as a string.\n",
    "    \"\"\"\n",
    "    # Compile a regular expression pattern to match the 'jobTitle' class\n",
    "    regex = re.compile('jobTitle *')\n",
    "    \n",
    "    # Find all 'h2' tags with the 'jobTitle' class and extract the text of the first tag\n",
    "    jobTitle = div.find_all('h2', class_=regex)[0].text\n",
    "    \n",
    "    # Return the job title as a string\n",
    "    return jobTitle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "44b9ecba-ceb8-4cae-a73a-87ba0754e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_attributes(div):\n",
    "    '''\n",
    "    This function creates a dictionary of job attributes from a job div.\n",
    "    \n",
    "    Args:\n",
    "        div: A BeautifulSoup object representing a job div.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the job attributes. If an attribute is absent, \n",
    "        it will not be added to the dictionary.\n",
    "    '''\n",
    "    # Create a dictionary of the job attributes\n",
    "    dic={}\n",
    "    # Remark\n",
    "    # In this function, we will use Try/Except to avoiding ending the loop because a error due \n",
    "    # to an absent attribute\n",
    "    \n",
    "    # Read the job title\n",
    "    try:\n",
    "        regex = re.compile('jobTitle *')\n",
    "        jobTitle = div.find_all('h2', class_=regex)[0].text\n",
    "        dic['jobTitle']=jobTitle\n",
    "    except:pass\n",
    "    # Read the job company name\n",
    "    try:\n",
    "        companyName=div.find_all('span', class_=\"companyName\")[0].text\n",
    "        dic['companyName']=companyName\n",
    "    except:pass\n",
    "    # Read the job location\n",
    "    try:\n",
    "        location=div.find_all('div', class_=\"jobsearch-PreciseLocation-container\")[0].text\n",
    "        dic['location']=location\n",
    "    except:\n",
    "        try: \n",
    "            location=div.find_all('div', class_=\"companyLocation\")[0].text\n",
    "            dic['location']=location\n",
    "        except:pass\n",
    "    # Read the job salary\n",
    "    try:\n",
    "        regex = re.compile('metadata salary*')\n",
    "        salary=div.find_all('div', class_=regex)[0].text\n",
    "        dic['salary']=salary\n",
    "    except:pass\n",
    "    # Read other metadata\n",
    "    try:  \n",
    "        other_metadata=div.find_all('div', class_='metadata').text\n",
    "        dic['other_metadata']=other_metadata\n",
    "    except:pass\n",
    "    # Read the job date\n",
    "    try:\n",
    "        date=div.find_all('span',class_=\"date\")[0].text\n",
    "        dic['date']=date\n",
    "    except:pass\n",
    "    # Return the dictionary of job attributes\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f9dda2de-25e7-48b3-9b75-c17b7e8d3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data as csv file\n",
    "def save_data_as_csv(data,page,run_id):\n",
    "    # Print the saving task\n",
    "    print('save CSV')\n",
    "    # Convert the list of dictionaries to Pandas DataFrame\n",
    "    df=pd.DataFrame(data)\n",
    "    # Save the dataFrame as csv file\n",
    "    file_name='data\\\\raw\\\\data'+str(page)+'_run_id_'+run_id+'.csv'\n",
    "    df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c9975c1-bbd5-4c87-ab3a-6033d3b3c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_the_final_page(soup):\n",
    "    return len(soup.find_all('a',{'aria-label': [\"Next Page\"]})) ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7b53a659-7fdb-46b0-94d2-1dafd020c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the next page\n",
    "def next_page(page):\n",
    "    # Make the next page url \n",
    "    next_url='https://fr.indeed.com/jobs?q='+job_topic_str.replace(' ','%20')+'&l='+\\\n",
    "            where_to_find_job.replace(' ','%20')+'&start='+str(page+1)+'0'\n",
    "    # Go to the next page\n",
    "    driver.get(next_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ff442-2449-4466-be19-f82783f4e483",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "424c8e88-9bbf-4e98-9b03-7cccf7229e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the chrome browser \n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "# Enter Indeed website\n",
    "driver.get(\"https://fr.indeed.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302c9bc-cbfa-4d13-9423-ae8f82093a6e",
   "metadata": {},
   "source": [
    "<img src='images\\\\img1.PNG'  width='400'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "91da57ff-5dd7-4738-859c-6d065753c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the search topic to \"data scientist\"\n",
    "job_topic_str=\"data scientist\"\n",
    "\n",
    "# Call the job_topic function to enter the search topic into the input box \"What\"\n",
    "job_topic(job_topic_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f46f7e63-160b-459f-8fce-330db2eeb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the location of the search to \"France\"\n",
    "where_to_find_job=\"France\"\n",
    "\n",
    "# Call the job_localisation function to enter the location of the search into the input box \"Where\"\n",
    "job_localisation(where_to_find_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "460afb63-4619-44a9-b3d9-2e580187349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and click to the search button\n",
    "driver.find_element(By.XPATH,\"//button[text()='Rechercher']\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e013e-34e7-4861-83d3-6d478ebe272f",
   "metadata": {},
   "source": [
    "<img src='images\\\\img2.PNG'  width='400'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "48b37328-8783-40b6-aa0f-f7c639de05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glbal variables initialisation \n",
    "data=[]\n",
    "# The maximum number of job to search\n",
    "Nb_max_jobs=2000\n",
    "# The maximum jobs/rows by csv file\n",
    "Nb_job_by_csv=100\n",
    "# The page number\n",
    "page=0\n",
    "# The job number \n",
    "jobs=0\n",
    "# This number is like a run ID\n",
    "# it is a random integer to avoir overwrinting the CSV file if we run the PYTHON notbock two time \n",
    "# We hope that the random number is not the same for both runs \n",
    "# Alternative solution: adding the date _ time to the csv file. \n",
    "run_id=str(np.random.randint(0,10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a6b3b-8b17-46da-bef1-29f6b8ed1871",
   "metadata": {},
   "source": [
    "# The main loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce92a75-ae3b-4625-ab69-dd49521d6017",
   "metadata": {},
   "source": [
    "This is a Python loop that reads job divs from a webpage and extracts the job attributes. The loop continues until the last page is read or the maximum number of tasks is reached. <br>The loop first calls the `find_all_job_divs` function to get all the job divs and the soup object. It then reads each job div using a `for` loop and extracts the job attributes using the `job_attributes` function. The function adds the page number and job number to the dictionary and appends the job dictionary to the list “data”. The loop then checks if the number of collected jobs exceeds the specified maximum number of jobs by CSV file. If it does, the data is saved as a CSV file and the list of jobs is initialized. The loop then checks if the final page is read or the maximum number of tasks is reached. If it is, the main loop ends with printing END. If not, the loop goes to the next page and increments the page counter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "91fab56e-95d0-4e69-acff-fd801eac27dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save CSV\n",
      "save CSV\n",
      "save CSV\n",
      "save CSV\n",
      "save CSV\n",
      "END of the main loop\n"
     ]
    }
   ],
   "source": [
    "# The main loop: continue until the last page is read or the maximum number of tasks is reached\n",
    "while(1):\n",
    "    # Get all the job divs and the soup object, in average each page has 10 job offres \n",
    "    divs_mosaic, soup=find_all_job_divs()\n",
    "    # Read each div\n",
    "    for i,div in enumerate(divs_mosaic):\n",
    "        # get the job attributes\n",
    "        dic=job_attributes(div)\n",
    "        # add the page number to the dictionnary \n",
    "        dic['page']=page\n",
    "        # add the job number to the dictionnary \n",
    "        dic['job']=jobs\n",
    "        # Append the job dictionary to the list \"data\"\n",
    "        data.append(dic)\n",
    "        # Increment the jobs counter\n",
    "        jobs+=1\n",
    "    # Check if the number of the collected jobs depasse the specify maximum number of jobs by CSV file\n",
    "    if len(data) >= Nb_job_by_csv:\n",
    "        # Save the data as csv file\n",
    "        save_data_as_csv(data,page,run_id)\n",
    "        # Intialise the list of jobs\n",
    "        data=[]\n",
    "    # Check if the final page is readed or or the maximum number of tasks is reached\n",
    "    end_pages=check_if_the_final_page(soup)\n",
    "    if jobs >= Nb_max_jobs or end_pages:\n",
    "        # If yes end the main loop with printing END\n",
    "        print('END of the main loop')\n",
    "        break\n",
    "    # go to the next page\n",
    "    next_page(page)\n",
    "    # Increments the page counter\n",
    "    page+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620cba3d-c84b-4786-bd87-1086e24ec1c5",
   "metadata": {},
   "source": [
    "# Group the csv files in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3749217e-742f-45d6-8d25-64ec64682784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\raw\\\\data6_run_id_4725.csv',\n",
       " 'data\\\\raw\\\\data13_run_id_4725.csv',\n",
       " 'data\\\\raw\\\\data20_run_id_4725.csv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find all CSV files in the \"data/raw\" directory\n",
    "files = glob.glob(\"data\\\\raw\\\\*.csv\")\n",
    "\n",
    "# Print the first three files\n",
    "display(files[:3])\n",
    "\n",
    "# Read all CSV files into a list of DataFrames\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "26fb3f20-2daf-4d52-aaba-c6597ec4a741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobTitle</th>\n",
       "      <th>companyName</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>page</th>\n",
       "      <th>job</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist H/F</td>\n",
       "      <td>Aon Corporation</td>\n",
       "      <td>Paris (75)+1 location</td>\n",
       "      <td>PostedOffre publiée il y a plus de 30 jours</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alternance - Data Scientist F/H</td>\n",
       "      <td>BPCE SA</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>PostedOffre publiée il y a plus de 30 jours</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          jobTitle      companyName               location   \n",
       "0               Data Scientist H/F  Aon Corporation  Paris (75)+1 location  \\\n",
       "1  Alternance - Data Scientist F/H          BPCE SA             Paris (75)   \n",
       "\n",
       "                                          date  page  job salary  \n",
       "0  PostedOffre publiée il y a plus de 30 jours     0    0    NaN  \n",
       "1  PostedOffre publiée il y a plus de 30 jours     0    1    NaN  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first 2 rows of the grouped dataFrame\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adebb43-d80a-4574-ae42-8dc4c23ebddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the \"Unnamed: 0\" column from the DataFrame\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Write the DataFrame to a CSV file without the index column\n",
    "df.to_csv('data\\\\processed\\\\all_scraped_files.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
